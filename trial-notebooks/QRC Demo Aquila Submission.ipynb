{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b03759a-6e66-4f91-af13-8f22f954d84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.69.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.69.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.12.23 gast-0.6.0 google-pasta-0.2.0 grpcio-1.69.0 h5py-3.12.1 keras-3.7.0 libclang-18.1.1 markdown-3.7 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 protobuf-5.29.2 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.0\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hCollecting bloqade\n",
      "  Downloading bloqade-0.15.14-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting juliacall>=0.9.14 (from bloqade)\n",
      "  Downloading juliacall-0.9.23-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: numpy>=1.25.2 in /opt/conda/lib/python3.11/site-packages (from bloqade) (1.26.4)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.11/site-packages (from bloqade) (2.10.3)\n",
      "Collecting scipy>=1.9.3 (from bloqade)\n",
      "  Downloading scipy-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pandas>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from bloqade) (2.2.3)\n",
      "Requirement already satisfied: bokeh>=3.2.2 in /opt/conda/lib/python3.11/site-packages (from bloqade) (3.6.2)\n",
      "Collecting tabulate>=0.9.0 (from bloqade)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting requests-sigv4>=0.1.6 (from bloqade)\n",
      "  Downloading requests_sigv4-0.1.6-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting amazon-braket-sdk>=1.78.0 (from bloqade)\n",
      "  Downloading amazon_braket_sdk-1.88.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting plotext>=5.2.8 (from bloqade)\n",
      "  Downloading plotext-5.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting beartype>=0.15.0 (from bloqade)\n",
      "  Downloading beartype-0.19.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting simplejson>=3.19.1 (from bloqade)\n",
      "  Downloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting plum-dispatch>=2.2.2 (from bloqade)\n",
      "  Downloading plum_dispatch-2.5.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting numba>=0.58.0 (from bloqade)\n",
      "  Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting amazon-braket-schemas>=1.21.3 (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading amazon_braket_schemas-1.22.4-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting amazon-braket-default-simulator>=1.26.0 (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading amazon_braket_default_simulator-1.26.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting oqpy~=0.3.5 (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading oqpy-0.3.7-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting backoff (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: boltons in /opt/conda/lib/python3.11/site-packages (from amazon-braket-sdk>=1.78.0->bloqade) (24.0.0)\n",
      "Collecting boto3>=1.28.53 (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading boto3-1.35.93-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting cloudpickle==2.2.1 (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.11/site-packages (from amazon-braket-sdk>=1.78.0->bloqade) (1.6.0)\n",
      "Collecting networkx (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting openpulse (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading openpulse-1.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting openqasm3 (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading openqasm3-1.0.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting sympy (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting backports.entry-points-selectable (from amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading backports.entry_points_selectable-1.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /opt/conda/lib/python3.11/site-packages (from bokeh>=3.2.2->bloqade) (3.1.4)\n",
      "Requirement already satisfied: contourpy>=1.2 in /opt/conda/lib/python3.11/site-packages (from bokeh>=3.2.2->bloqade) (1.3.1)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.11/site-packages (from bokeh>=3.2.2->bloqade) (24.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /opt/conda/lib/python3.11/site-packages (from bokeh>=3.2.2->bloqade) (11.0.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.11/site-packages (from bokeh>=3.2.2->bloqade) (6.0.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /opt/conda/lib/python3.11/site-packages (from bokeh>=3.2.2->bloqade) (6.4)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /opt/conda/lib/python3.11/site-packages (from bokeh>=3.2.2->bloqade) (2024.9.0)\n",
      "Collecting juliapkg~=0.1.8 (from juliacall>=0.9.14->bloqade)\n",
      "  Downloading juliapkg-0.1.15-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.58.0->bloqade)\n",
      "  Downloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1.0->bloqade) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1.0->bloqade) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1.0->bloqade) (2024.2)\n",
      "Requirement already satisfied: rich>=10.0 in /opt/conda/lib/python3.11/site-packages (from plum-dispatch>=2.2.2->bloqade) (13.9.4)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.11/site-packages (from plum-dispatch>=2.2.2->bloqade) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->bloqade) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->bloqade) (2.27.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from requests-sigv4>=0.1.6->bloqade) (2.31.0)\n",
      "Collecting requests-aws-sign (from requests-sigv4>=0.1.6->bloqade)\n",
      "  Downloading requests_aws_sign-0.1.6-py3-none-any.whl.metadata (938 bytes)\n",
      "Requirement already satisfied: opt_einsum in /opt/conda/lib/python3.11/site-packages (from amazon-braket-default-simulator>=1.26.0->amazon-braket-sdk>=1.78.0->bloqade) (3.4.0)\n",
      "Collecting antlr4-python3-runtime==4.9.2 (from amazon-braket-default-simulator>=1.26.0->amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading antlr4-python3-runtime-4.9.2.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting botocore<1.36.0,>=1.35.93 (from boto3>=1.28.53->amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading botocore-1.35.93-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.28.53->amazon-braket-sdk>=1.78.0->bloqade) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.28.53->amazon-braket-sdk>=1.78.0->bloqade) (0.10.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from Jinja2>=2.9->bokeh>=3.2.2->bloqade) (2.1.5)\n",
      "Collecting semver~=3.0 (from juliapkg~=0.1.8->juliacall>=0.9.14->bloqade)\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from oqpy~=0.3.5->amazon-braket-sdk>=1.78.0->bloqade) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->bloqade) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.0->plum-dispatch>=2.2.2->bloqade) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.0->plum-dispatch>=2.2.2->bloqade) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->requests-sigv4>=0.1.6->bloqade) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->requests-sigv4>=0.1.6->bloqade) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->requests-sigv4>=0.1.6->bloqade) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->requests-sigv4>=0.1.6->bloqade) (2024.2.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->amazon-braket-sdk>=1.78.0->bloqade)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.0->plum-dispatch>=2.2.2->bloqade) (0.1.2)\n",
      "Downloading bloqade-0.15.14-py3-none-any.whl (180 kB)\n",
      "Downloading amazon_braket_sdk-1.88.3-py3-none-any.whl (315 kB)\n",
      "Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading beartype-0.19.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading juliacall-0.9.23-py3-none-any.whl (12 kB)\n",
      "Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading plotext-5.3.2-py3-none-any.whl (64 kB)\n",
      "Downloading plum_dispatch-2.5.4-py3-none-any.whl (42 kB)\n",
      "Downloading requests_sigv4-0.1.6-py3-none-any.whl (4.8 kB)\n",
      "Downloading scipy-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading amazon_braket_default_simulator-1.26.2-py3-none-any.whl (223 kB)\n",
      "Downloading amazon_braket_schemas-1.22.4-py3-none-any.whl (127 kB)\n",
      "Downloading boto3-1.35.93-py3-none-any.whl (139 kB)\n",
      "Downloading juliapkg-0.1.15-py3-none-any.whl (16 kB)\n",
      "Downloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m139.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading oqpy-0.3.7-py3-none-any.whl (36 kB)\n",
      "Downloading openpulse-1.0.1-py3-none-any.whl (537 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m537.8/537.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openqasm3-1.0.0-py3-none-any.whl (539 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m539.8/539.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading backports.entry_points_selectable-1.3.0-py3-none-any.whl (6.2 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_aws_sign-0.1.6-py3-none-any.whl (3.0 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.35.93-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.2-py3-none-any.whl size=144548 sha256=74f2987b5120f59f67a83e2b85eb756acd5a5bdf809e8fef5a83e68345984b7d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/02/5e/de/90c5aab11b66f94cf3f381ec55aaf3f2008213a7056310e25f\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: openqasm3, mpmath, antlr4-python3-runtime, tabulate, sympy, simplejson, semver, scipy, plotext, networkx, llvmlite, cloudpickle, beartype, backports.entry-points-selectable, backoff, openpulse, numba, juliapkg, botocore, plum-dispatch, oqpy, juliacall, amazon-braket-schemas, boto3, amazon-braket-default-simulator, requests-aws-sign, amazon-braket-sdk, requests-sigv4, bloqade\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.1.0\n",
      "    Uninstalling cloudpickle-3.1.0:\n",
      "      Successfully uninstalled cloudpickle-3.1.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.76\n",
      "    Uninstalling botocore-1.35.76:\n",
      "      Successfully uninstalled botocore-1.35.76\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask 2024.12.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "distributed 2024.12.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "awscli 1.36.17 requires botocore==1.35.76, but you have botocore 1.35.93 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed amazon-braket-default-simulator-1.26.2 amazon-braket-schemas-1.22.4 amazon-braket-sdk-1.88.3 antlr4-python3-runtime-4.9.2 backoff-2.2.1 backports.entry-points-selectable-1.3.0 beartype-0.19.0 bloqade-0.15.14 boto3-1.35.93 botocore-1.35.93 cloudpickle-2.2.1 juliacall-0.9.23 juliapkg-0.1.15 llvmlite-0.43.0 mpmath-1.3.0 networkx-3.4.2 numba-0.60.0 openpulse-1.0.1 openqasm3-1.0.0 oqpy-0.3.7 plotext-5.3.2 plum-dispatch-2.5.4 requests-aws-sign-0.1.6 requests-sigv4-0.1.6 scipy-1.15.0 semver-3.0.2 simplejson-3.19.3 sympy-1.13.3 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install sklearn\n",
    "!pip install bloqade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278dbd23-bee8-4d9a-8c12-754944e0c65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Collecting scikit-learn',\n",
       " '  Downloading scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)',\n",
       " 'Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)',\n",
       " 'Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.15.0)',\n",
       " 'Collecting joblib>=1.2.0 (from scikit-learn)',\n",
       " '  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)',\n",
       " 'Collecting threadpoolctl>=3.1.0 (from scikit-learn)',\n",
       " '  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)',\n",
       " 'Downloading scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)',\n",
       " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/13.5 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
       " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m13.5/13.5 MB\\x1b[0m \\x1b[31m92.5 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
       " '\\x1b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)',\n",
       " 'Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)',\n",
       " 'Installing collected packages: threadpoolctl, joblib, scikit-learn',\n",
       " 'Successfully installed joblib-1.4.2 scikit-learn-1.6.0 threadpoolctl-3.5.0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d892ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 14:47:49.153699: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-07 14:47:49.159681: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-07 14:47:49.166435: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-07 14:47:49.179901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736261269.199912     874 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736261269.206006     874 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 14:47:49.229683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5137b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bloqade\n",
    "from bloqade.ir import Chain, start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038e9ca-8a29-4b8d-993e-17fcb3470308",
   "metadata": {},
   "source": [
    "# Quantum Reservoir Computing Demo\n",
    "\n",
    "In this notebook we will show you how to train a model to classify MNIST images using quantum reservoir computing (QRC) with Bloqade. The general idea is that the chaotic, non-linear dynamics of quantum systems can be used to create a non-linear mapping from a low-dimensional space (which will be PCA embeddings extracted from the MNIST images) to a high dimensional space. In order to construct the mapping, we first ensure that information from the image will totally specify the dynamics of the quantum system. Then, we measure certain properties (in this case, spin expectations and correlations) over time. These measurements form the output of the mapping.   \n",
    "\n",
    "Through the embedding in this high dimensional space classification on using standard ML techniques is expected to perform better. \n",
    "\n",
    "This demo provides a toy model that does not require access to a quantum computer. It uses classical, numerical simulations of a quantum system with 8 qubits. The classical simulation makes use of the package Bloqade, which is designed for numerical simulations of neutral-atom architectures. With only a minimal of changes, we can use the same code to submit our jobs to Aquila hardware and analyse results.\n",
    "\n",
    "Neutral atom architectures provide access to several variational parameters. In this demo, we map pixelated descriptions of the images to the local detuning terms in the Rydberg Hamiltonian. We then measure the quantum system and train the classical model which maps the quantum readout from the measurement to the desired prediction. As all training process is in the classical part (excuted on a classical computer), we expect the training is substantially shorter than other traditional quantum machine learning models. More details on numerical simulation of neutral-atom architectures, and the range of the parameters can be found from [Bloqade](https://github.com/QuEraComputing/bloqade-python) documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea12b36-2f1c-4538-8cec-c365091a7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "X_train: (60000, 28, 28)\n",
      "Y_train: (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Download the MNIST dataset and rescaling data\n",
    "                     \n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X, test_X = train_X / 255.0, test_X / 255.0\n",
    "\n",
    "## training data set (each image has 28x28 pixels and there are 60000 training samples)\n",
    "print('X_train: ' + str(train_X.shape))\n",
    "print('Y_train: ' + str(train_y.shape))\n",
    "\n",
    "## test data set (10000 test samples)\n",
    "print('X_test:  '  + str(test_X.shape))\n",
    "print('Y_test:  '  + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122d68ab-68f0-4d82-a0a1-3217d480bbba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of an training dataset image for '5'\n",
    "plt.imshow(train_X[0], cmap=plt.get_cmap('gray'), interpolation='None')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958cbc65-6686-42c5-9883-389e4498d14d",
   "metadata": {},
   "source": [
    "## PCA Reduction\n",
    "\n",
    "In this notebook, we first train a toy model using 1000 samples from the MNIST dataset.\n",
    "\n",
    "The images form the input into the quantum system. As we focus on numerical simulation where the number of atoms in the quantum system is limited, we first perform dimensionality reduction using the [principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc2f07f-b04f-4e06-946f-1c7baef0a677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set PCA: \n",
      "  [[ 0.48601015  1.22617358  0.09613354  2.17944297 -0.10704576  0.9116717\n",
      "   0.91763033  0.62666468]\n",
      " [ 3.96752304  1.15630211 -2.33858651  1.80692626 -3.24421656  0.71353148\n",
      "  -0.17655089 -0.41164546]\n",
      " [-0.2033318  -1.53793393  0.73925392 -2.04318175 -1.20266952  0.00719743\n",
      "  -3.36881255  1.44545833]\n",
      " [-3.13383152  2.38116556 -1.07314212 -0.41520877 -0.00726755 -2.74374391\n",
      "  -1.85769884 -0.2640067 ]\n",
      " [-1.50099977 -2.86487399 -0.06413234  0.94783341  0.38494646 -0.16952834\n",
      "  -0.35947686 -1.59041131]] \n",
      "\n",
      "Test set PCA: \n",
      "  [[ 0.48601015  1.22617358  0.09613354  2.17944297 -0.10704576  0.9116717\n",
      "   0.91763033  0.62666468]\n",
      " [ 3.96752304  1.15630211 -2.33858651  1.80692626 -3.24421656  0.71353148\n",
      "  -0.17655089 -0.41164546]\n",
      " [-0.2033318  -1.53793393  0.73925392 -2.04318175 -1.20266952  0.00719743\n",
      "  -3.36881255  1.44545833]\n",
      " [-3.13383152  2.38116556 -1.07314212 -0.41520877 -0.00726755 -2.74374391\n",
      "  -1.85769884 -0.2640067 ]\n",
      " [-1.50099977 -2.86487399 -0.06413234  0.94783341  0.38494646 -0.16952834\n",
      "  -0.35947686 -1.59041131]] \n"
     ]
    }
   ],
   "source": [
    "# We first use PCA to downsample the data into 10-dimensional vectors\n",
    "dim_pca = 8\n",
    "\n",
    "# Use the `fit` function from the `sklearn` package to define the PCA model and apply to training set\n",
    "pca=PCA(n_components=dim_pca).fit(np.reshape(train_X, (60000,28*28)))\n",
    "x=pca.transform(np.reshape(train_X, (60000,28*28)))\n",
    "\n",
    "# Let us see how it looks\n",
    "num_examples = 1000\n",
    "xs = x[:num_examples,:]\n",
    "print(\"Training set PCA: \\n \", xs[0:5,:], \"\\n\")\n",
    "\n",
    "#processing the test set\n",
    "xt=pca.transform(np.reshape(test_X, (10000,28*28)))\n",
    "\n",
    "# Let us see how it looks\n",
    "num_test_examples = 200\n",
    "xs_test = xt[:num_test_examples,:]\n",
    "print(\"Test set PCA: \\n \", xs[0:5,:], \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8dd2d9",
   "metadata": {},
   "source": [
    "Here, we scale the range of principal components to a feasible range for local detuning implementation, [0, 1]. And later, for each image, we will encode each of the 8 scaled principal components into each single local detuning for 8 atoms. For more details of the neutral atom quantum system, please refer to the documentation of [Bloqade](https://queracomputing.github.io/Bloqade.jl/dev/#What-does-Bloqade-Do?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed5a7bc3-9d38-4a86-bb7e-ad8be8eb481e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spectral =(np.amax(xs) - np.amin(xs))\n",
    "m1=np.amin(xs)\n",
    "xs = (xs - m1)/spectral # to make sure values to be between [0, 1]\n",
    "xs_test = (xs_test - m1)/spectral # the same transformation on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e3094-94c8-4dce-b631-660a094c7475",
   "metadata": {},
   "source": [
    "## Build quantum tasks and simulate dynamics\n",
    "\n",
    "We can now set up the quantum simulation. \n",
    "\n",
    "- Define a `dictionary`, which captures all physical parameters and readouts of the quantum system.\n",
    "- Define functions that simulate quantum dynamics or build tasks that can me submitted to hardware with the input `x` from scaled detunings \n",
    "- The simulation of the quantum dynamics has been implemented in [Bloqade](https://github.com/QuEraComputing/bloqade-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e67796a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "QRC_parameters={\n",
    "    \"atom_number\":dim_pca, #number of atoms, equal to the dimension of PCA feature vector\n",
    "    \"geometry_spec\":Chain(dim_pca, lattice_spacing=10), #atom geometry - we will use a linear chain with 10 micron distance between atoms\n",
    "    \"encoding_scale\":9.0, #scaling factor for local detuning encoding\n",
    "    \"rabi_frequency\":6.283,#value of Rabi frequency used\n",
    "    \"total_time\":4, #total maximum evolution time - 4 microseconds\n",
    "    \"time_steps\":8, #number of probe times for quantum embedding collection - 8 in the 4 microsecond window\n",
    "    \"readouts\":\"ZZ\", #includes both ZZ correlators together with default Rydberg density as generated embeddings\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1e17c",
   "metadata": {},
   "source": [
    "Here, we define the function that will proccess the QRC parameters define above and return a task that can be either simulated or ran on hardware. In addition, we define the processing pipeline that will turn the samples collected on hardware or in simulation into QRC embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ffae4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function for building quantum tasks.\n",
    "def build_task(QRC_parameters, xs1):\n",
    "    natoms=QRC_parameters[\"atom_number\"]\n",
    "    encoding_scale=QRC_parameters[\"encoding_scale\"]\n",
    "    dt=QRC_parameters[\"total_time\"]/QRC_parameters[\"time_steps\"]\n",
    "    #builds global Rabi and detuning pulses\n",
    "    rabi_oscillations_program = (QRC_parameters[\"geometry_spec\"]\n",
    "            .rydberg.rabi.amplitude.uniform.constant(\n",
    "                duration=\"run_time\", value=QRC_parameters[\"rabi_frequency\"]\n",
    "            )\n",
    "            .detuning.uniform.constant(duration=\"run_time\", value=encoding_scale/2)\n",
    "            #adds local detuning according to the feature vector\n",
    "            .scale(list(xs1)).constant(duration=\"run_time\", value=-encoding_scale)\n",
    "            ) \n",
    "    rabi_oscillation_job = rabi_oscillations_program.batch_assign(run_time=np.arange(1, QRC_parameters[\"time_steps\"]+1, 1)*dt)\n",
    "    #`batch_assign` used to probe the quantum reservoir at set number of timesteps\n",
    "    return rabi_oscillation_job\n",
    "\n",
    "#To obtain the embeddings, we process the report containing the collected samples into embeddins made of Z and ZZ observables.\n",
    "def process_results(QRC_parameters, report):  \n",
    "    embedding=[]\n",
    "    natoms=QRC_parameters[\"atom_number\"]\n",
    "    try:\n",
    "        for t in range(QRC_parameters[\"time_steps\"]):\n",
    "            ar1=-1.0+2*((report.bitstrings())[t])\n",
    "            nsh1=ar1.shape[0]\n",
    "            for i in range(natoms):\n",
    "                embedding.append(np.sum(ar1[:,i])/nsh1) #Z expectation values\n",
    "            if QRC_parameters[\"readouts\"]==\"ZZ\":\n",
    "                for i in range(natoms):\n",
    "                    for j in range(i+1,natoms):\n",
    "                        embedding.append(np.sum(ar1[:,i]*ar1[:,j])/nsh1) #ZZ expectation values\n",
    "    except: #In case no experimental results were obtained.\n",
    "        print(\"No results exist.\")\n",
    "        for t in range(QRC_parameters[\"time_steps\"]):\n",
    "            for i in range(natoms):\n",
    "                embedding.append(0.0)\n",
    "            if QRC_parameters[\"readouts\"]==\"ZZ\":\n",
    "                for i in range(natoms):\n",
    "                    for j in range(i+1,natoms):\n",
    "                        embedding.append(0.0)\n",
    "    return embedding\n",
    "\n",
    "#Processing if only samples are needed.\n",
    "def process_results_samples(QRC_parameters, report):  \n",
    "    embedding=[]\n",
    "    natoms=QRC_parameters[\"atom_number\"]\n",
    "    try:\n",
    "        embedding=report.bitstrings()\n",
    "        # for t in range(QRC_parameters[\"time_steps\"]):\n",
    "        #     ar1=-1.0+2*((report.bitstrings())[t])\n",
    "        #     nsh1=ar1.shape[0]\n",
    "        #     for i in range(natoms):\n",
    "        #         embedding.append(np.sum(ar1[:,i])/nsh1) #Z expectation values\n",
    "        #     if QRC_parameters[\"readouts\"]==\"ZZ\":\n",
    "        #         for i in range(natoms):\n",
    "        #             for j in range(i+1,natoms):\n",
    "        #                 embedding.append(np.sum(ar1[:,i]*ar1[:,j])/nsh1) #ZZ expectation values\n",
    "    except: #In case no experimental results were obtained.\n",
    "        print(\"No results exist.\")\n",
    "        for t in range(QRC_parameters[\"time_steps\"]):\n",
    "            for i in range(natoms):\n",
    "                embedding.append(0.0)\n",
    "            if QRC_parameters[\"readouts\"]==\"ZZ\":\n",
    "                for i in range(natoms):\n",
    "                    for j in range(i+1,natoms):\n",
    "                        embedding.append(0.0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51321f29",
   "metadata": {},
   "source": [
    "In order to generate QRC embeddings, we will call emulation routines for the tasks we build and process the resulting data. We will collect 1000 samples per datapoint in emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea54345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Simulating and gathering samples only\n",
    "# samples_train=np.array([process_results_samples(QRC_parameters, \n",
    "#         build_task(QRC_parameters, xs[data,:]).bloqade.python().run(shots=100, rtol=1e-8, atol=1e-8).report())\n",
    "#         for data in range(num_examples)])\n",
    "# np.save(\"samples_train.npy\", samples_train)\n",
    "# samples_test=np.array([process_results_samples(QRC_parameters, \n",
    "#         build_task(QRC_parameters, xs_test[data,:]).bloqade.python().run(shots=100, rtol=1e-8, atol=1e-8).report())\n",
    "#         for data in range(num_test_examples)])\n",
    "# np.save(\"samples_test.npy\", samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43caeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_emulation(xs, num_examples, nshots=1000):\n",
    "    return np.array([process_results(QRC_parameters, \n",
    "        build_task(QRC_parameters, xs[data,:]).bloqade.python().run(shots=nshots, rtol=1e-8, atol=1e-8).report())\n",
    "        for data in range(num_examples)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a553c9b",
   "metadata": {},
   "source": [
    "Now, we are ready to run the simulation by apply the defined emulation pipeline to the scaled feature vectors! For each image, the readouts from the quantum dynamics is a 288-dimensional vector which has much higher dimension than the PCA dimension. The full results will be stored in a $1000\\times 288$ matrix `embeddings`.\n",
    "\n",
    "(It might take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5935be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=get_embeddings_emulation(xs, num_examples, nshots=1000)\n",
    "test_embeddings=get_embeddings_emulation(xs_test, num_test_examples, nshots=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "067eac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 288)\n",
      "(200, 288)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)\n",
    "print(test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc61b5-2e5e-4954-8cc9-eccacc34c1f1",
   "metadata": {},
   "source": [
    "## Training Neural Network and Evaluating Performance\n",
    "\n",
    "Now we are going to train a classical neural network without hidden layers (a linear classifier with `softmax` output function), using quantum measurements stored in `embeddings` as the input. In comparision, we also trained a neural network without hidden layers directly using PCA feature vectors without the quantum reservoir processing, as well as a simple neural network with two hidden layers for comparison.\n",
    "\n",
    "Here, we used the machine learning framework of [tensorflow](https://www.tensorflow.org/) for the neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a971b",
   "metadata": {},
   "source": [
    "First, let's see the results of a linear classifier applied on PCA features directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe750ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 15:34:16.954387: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "#building a linear model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting to train data\n",
    "model.fit(xs, train_y[:num_examples], epochs=1000, batch_size=100, verbose=0)\n",
    "\n",
    "#evaluating on test data\n",
    "test_loss, test_acc = model.evaluate(xs_test,  test_y[:num_test_examples], verbose=0)\n",
    "print('PCA test accuracy:', 100*round(test_acc, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3e316",
   "metadata": {},
   "source": [
    "And now, let's apply the same linear classifier on our QRC embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ffd5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QRC test accuracy: 82.0 %\n"
     ]
    }
   ],
   "source": [
    "#building a linear model\n",
    "#we include regularization and tune epsilon parameter of the optimizer to better control training from QRC embeddings generated on finite number of samples\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.L1(l1=0.0001))\n",
    "    ])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(epsilon=0.0002),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#fitting to train data\n",
    "model.fit(embeddings, train_y[:num_examples], epochs=2000, batch_size=100, verbose=0)\n",
    "\n",
    "#evaluating on test data\n",
    "test_loss, test_acc = model.evaluate(test_embeddings,  test_y[:num_test_examples], verbose=0)\n",
    "print('QRC test accuracy:', 100*round(test_acc, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8b956",
   "metadata": {},
   "source": [
    "QRC embeddings significantly outperform linear classifier on PCA embeddings only!\n",
    "\n",
    "Finally, let's compare with a sizable classical neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266a1d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-layer NN test accuracy: 83.0 %\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(10),\n",
    "\n",
    "    ])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(xs, train_y[:num_examples], epochs=1000, batch_size=100, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(xs_test,  test_y[:num_test_examples], verbose=0)\n",
    "print('4-layer NN test accuracy:', 100*round(test_acc, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe25659",
   "metadata": {},
   "source": [
    "While in this case classical neural network outperforms QRC, this is a consequence of limited dataset size. With more data, QRC will match the 4-layer neural network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5333d324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM test accuracy: 70.0 %\n",
      "SVC test accuracy (rbf kernel): 89.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QRC test accuracy: 82.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#The same implemented with LinearSVC and SVC from sklearn.\n",
    "\n",
    "svm = LinearSVC(C=1.0, multi_class='crammer_singer', dual=False)\n",
    "svm.fit(xs, train_y[:num_examples])\n",
    "y_pred = svm.predict(xs_test)\n",
    "accuracy=svm.score(xs_test, test_y[:num_test_examples])\n",
    "print('Linear SVM test accuracy:', 100*round(accuracy, 2), \"%\")\n",
    "\n",
    "svm = SVC(C=8.)\n",
    "svm.fit(xs, train_y[:num_examples])\n",
    "y_pred = svm.predict(xs_test)\n",
    "accuracy=svm.score(xs_test, test_y[:num_test_examples])\n",
    "print('SVC test accuracy (rbf kernel):', 100*round(accuracy, 2), \"%\")\n",
    "\n",
    "\n",
    "svm = LinearSVC(C=1.0, multi_class='crammer_singer', dual=False)\n",
    "svm.fit(embeddings, train_y[:num_examples])\n",
    "y_pred = svm.predict(embeddings)\n",
    "accuracy=svm.score(test_embeddings, test_y[:num_test_examples])\n",
    "print('QRC test accuracy:', 100*round(accuracy, 2), \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
